import argparse
import logging
import os
import numpy as np
import pandas as pd
import pytorch_lightning as pl
import torch
from pytorch_lightning import loggers as pl_loggers
from torch.utils.data import DataLoader, Dataset
import numpy as np
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
from dataset import KoGPTSummaryDataset
from transformers.optimization import AdamW, get_cosine_schedule_with_warmup
from soft_embedding import SoftEmbedding
import pdb

parser = argparse.ArgumentParser(description='KoGPT2 Summarization')

parser.add_argument('--checkpoint_path',
                    type=str,
                    help='checkpoint path')

logger = logging.getLogger()
logger.setLevel(logging.INFO)


class ArgsBase():
    @staticmethod
    def add_model_specific_args(parent_parser):
        parser = argparse.ArgumentParser(
            parents=[parent_parser], add_help=False)
        parser.add_argument('--train_file',
                            type=str,
                            default='data/train.tsv',
                            help='train file')

        parser.add_argument('--test_file',
                            type=str,
                            default='data/test.tsv',
                            help='test file')

        parser.add_argument('--batch_size',
                            type=int,
                            default=14,
                            help='')
        parser.add_argument('--max_len',
                            type=int,
                            default=1024,
                            help='max seq len')
        return parser

class KoGPTSummaryModule(pl.LightningDataModule):
    def __init__(self, train_file,
                 test_file, tok,
                 max_len=1024,
                 batch_size=8,
                 num_workers=4,
                 ptuning_n_token=0):
        super().__init__()
        self.batch_size = batch_size
        self.max_len = max_len
        self.train_file_path = train_file
        self.test_file_path = test_file
        self.ptuning_n_token = ptuning_n_token
        if tok is None:
            self.tok = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
                       bos_token='</s>', eos_token='</s>', unk_token='<unk>',
                       pad_token='<pad>', mask_token='<mask>') 
        else:
            self.tok = tok
        self.num_workers = num_workers

    @staticmethod
    def add_model_specific_args(parent_parser):
        parser = argparse.ArgumentParser(
            parents=[parent_parser], add_help=False)
        parser.add_argument('--num_workers',
                            type=int,
                            default=5,
                            help='num of worker for dataloader')
        return parser

    # OPTIONAL, called for every GPU/machine (assigning state is OK)
    def setup(self, stage):
        # split dataset
        self.train = KoGPTSummaryDataset(file=self.train_file_path,
                                 tok=self.tok,
                                 max_len=self.max_len,
                                 ptuning_n_token=self.ptuning_n_token)
        
        self.test = KoGPTSummaryDataset(file=self.test_file_path,
                                tok=self.tok,
                                max_len=self.max_len,
                                ptuning_n_token=self.ptuning_n_token)

    def train_dataloader(self):
        train = DataLoader(self.train,
                           batch_size=self.batch_size,
                           num_workers=self.num_workers, shuffle=True)
        return train

    def val_dataloader(self):
        val = DataLoader(self.test,
                         batch_size=self.batch_size,
                         num_workers=self.num_workers, shuffle=False)
        return val

    def test_dataloader(self):
        test = DataLoader(self.test,
                          batch_size=self.batch_size,
                          num_workers=self.num_workers, shuffle=False)
        return test


class Base(pl.LightningModule):
    def __init__(self, hparams, **kwargs) -> None:
        super(Base, self).__init__()
        self.hparams = hparams

    @staticmethod
    def add_model_specific_args(parent_parser):
        # add model specific args
        parser = argparse.ArgumentParser(
            parents=[parent_parser], add_help=False)

        parser.add_argument('--lr',
                            type=float,
                            default=3e-5,
                            help='The initial learning rate')

        parser.add_argument('--warmup_ratio',
                            type=float,
                            default=0.1,
                            help='warmup ratio')

        parser.add_argument('--model_path',
                            type=str,
                            default=None,
                            help='kobart model path')
        
        parser.add_argument('--prompt_length',
                            type=int,
                            default=20,
                            help='number of trainable parameters')
        
        return parser

    def configure_optimizers(self):
        # Prepare optimizer
#         param_optimizer = list(self.model.named_parameters())
#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
#         optimizer_grouped_parameters = [
#             {'params': [p for n, p in param_optimizer if not any(
#                 nd in n for nd in no_decay)], 'weight_decay': 0.01},
#             {'params': [p for n, p in param_optimizer if any(
#                 nd in n for nd in no_decay)], 'weight_decay': 0.0}
#         ]
        
        prompt_parameters = self.model.get_input_embeddings().learned_embedding
#         pdb.set_trace()
        optimizer = AdamW([prompt_parameters],
                          lr=self.hparams.lr, correct_bias=False)
        # warm up lr
        num_workers = self.hparams.num_workers
        data_len = len(self.train_dataloader().dataset)
        logging.info(f'number of workers {num_workers}, data length {data_len}')
        num_train_steps = int(data_len / (self.hparams.batch_size * num_workers) * self.hparams.max_epochs)
        logging.info(f'num_train_steps : {num_train_steps}')
        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)
        logging.info(f'num_warmup_steps : {num_warmup_steps}')
        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)
        lr_scheduler = {'scheduler': scheduler, 
                        'monitor': 'loss', 'interval': 'step',
                        'frequency': 1}
        return [optimizer], [lr_scheduler]


class KoGPTConditionalGeneration(Base):
    def __init__(self, hparams, **kwargs):
        super(KoGPTConditionalGeneration, self).__init__(hparams, **kwargs)
        self.model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
        
        for p in self.model.parameters():
            p.requires_grad = False
            
        self.hparams = hparams
        self.update_embedding()
        self.pad_token_id = 0
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
                       bos_token='</s>', eos_token='</s>', unk_token='<unk>',
                       pad_token='<pad>', mask_token='<mask>') 
    
        self.neg = 0
        self.loss_function = torch.nn.CrossEntropyLoss(reduction='none')
        
    
    def update_embedding(self):
        s_wte = SoftEmbedding(self.model.get_input_embeddings(), 
                      prompt_length=self.hparams.prompt_length, 
                      initialize_from_vocab=True)
        self.model.set_input_embeddings(s_wte)

    def forward(self, inputs):
        output = self.model(inputs, return_dict=True)
        return output.logits
        

    def training_step(self, batch, batch_idx):
        token_ids = batch['input']
        mask = batch['mask']
        label = batch['label']

        out = self(token_ids)        
        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)
        mask_out = torch.where(mask_3d == 1, out, self.neg * torch.ones_like(out))
        loss = self.loss_function(mask_out.transpose(2, 1), label)
        loss_avg = loss.sum() / mask.sum()
        self.log('train_loss', loss_avg)
        return loss_avg

    def validation_step(self, batch, batch_idx):
        token_ids = batch['input']
        mask = batch['mask']
        label = batch['label']
        
        
        
        out = self(token_ids)
        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)
        mask_out = torch.where(mask_3d == 1, out, self.neg * torch.ones_like(out))
        loss = self.loss_function(mask_out.transpose(2, 1), label)
        loss_avg = loss.sum() / mask.sum()
        return (loss_avg)

    def validation_epoch_end(self, outputs):
        losses = []
        for loss in outputs:
            losses.append(loss)
        self.log('val_loss', torch.stack(losses).mean(), prog_bar=True)

if __name__ == '__main__':
    parser = Base.add_model_specific_args(parser)
    parser = ArgsBase.add_model_specific_args(parser)
    parser = KoGPTSummaryModule.add_model_specific_args(parser)
    parser = pl.Trainer.add_argparse_args(parser)
    args = parser.parse_args()
    logging.info(args)

    model = KoGPTConditionalGeneration(args)

    dm = KoGPTSummaryModule(args.train_file,
                        args.test_file,
                        None,
                        batch_size=args.batch_size,
                        max_len=args.max_len,
                        num_workers=args.num_workers)
    
    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss',
                                                       dirpath=args.default_root_dir,
                                                       filename='model_chp/{epoch:02d}-{val_loss:.3f}',
                                                       verbose=True,
                                                       save_last=True,
                                                       mode='min',
                                                       save_top_k=3,
                                                       prefix='KoGPT2_summary')
    tb_logger = pl_loggers.TensorBoardLogger(os.path.join(args.default_root_dir, 'tb_logs'))
    lr_logger = pl.callbacks.LearningRateMonitor()
    trainer = pl.Trainer.from_argparse_args(args, logger=tb_logger,
                                            callbacks=[checkpoint_callback, lr_logger])
    trainer.fit(model, dm)
